{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Demo\n",
    "\n",
    "The support vector machine (SVM) is an example of a convex optimization problem w/ quadratic criteria. SVM is a technique for classifying data that may not be linearly separable. This is accomplished by performing a linear classification problem in a higher dimensional feature space where the data is linearly separable. \n",
    "\n",
    "In order to demonstrate how an SVM works, we walk through an example using `sklearn`. We then cast the SVM as a problem that can be solved via convex optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the decision function for a 2D SVM\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    \n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "    \n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k', \n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example SVM Problem\n",
    "\n",
    "First, let's assume we have $N$ tuples, $(x_i, y_i)$, where $x_i \\in R^m$ represent feature vectors, and $y_i$ represents the true class, $y_i \\in \\{-1, 1\\}$. For instance, in the figure below, the blue dots correspond to $x_i$ from $y_i = -1$, and the red dots correspond to features from $y_i = 1$. Let's also we've already done some kind of projection into a space where the classes are linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2 = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=.75)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=X2, s=50, edgecolors='black', cmap=plt.cm.coolwarm, alpha=0.6)\n",
    "plt.title(\"Data from Two Distinct Classes\")\n",
    "plt.gcf().set_size_inches((7, 7))    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only have access to each value of $x_i$, the class they belong to, the corresponding $y_i$, is unknown. \n",
    "Let's define a hyperplane by: \n",
    "\n",
    "$\\{ x: f(x) = x^T\\beta + 1 = 0 \\}$\n",
    "\n",
    "A classification rule induced by this hyperplane $f(x)$ is:\n",
    "\n",
    "$ G(x) = sign[x^T\\beta +1 ]$\n",
    "\n",
    "Since we are in a space where the classes are linearly separable, we can find a function $f(x) = x^T\\beta + 1 $ with $y_if(x_i) > 0 \\forall i$. Hence, we are able to find a hyperplane that creates the largest margin between the two classes. This is what the SVM accomplishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create svm using a linear kernel\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(X1, X2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=X2, s=50, edgecolors='black', cmap='coolwarm', alpha=0.6)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.title(\"SVM Classifier\")\n",
    "plt.gcf().set_size_inches((7, 7))    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circled points on the dotted lines are the so-called support vectors. The points between the dashed line and the decision boundary are within the margin. The SVM seeks to find the hyperplane that creates the largest margin, subject to the constraint to minimize the total distance of points on the wrong side of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM as a Quadratic Program\n",
    "\n",
    "We can solve classification problems more efficiently by re-writing the problem as one from convex optimization. Traditionally, the SVM is captured by the following:\n",
    "\n",
    "$ \\underset{\\beta}{\\operatorname{max}} M $\n",
    "\n",
    "subject to $y_i(x_i^T\\beta + 1) \\geq M, i=1, \\dots N$ \n",
    "\n",
    "Where, $M$ denotes the width of the margin. We can re-write this has\n",
    "\n",
    "$ \\underset{\\beta}{\\operatorname{min}} \\lVert \\beta \\rVert$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$y_i(x_i^T\\beta + 1) \\geq 1, i=1, \\dots N$, where $M = 1/\\lVert \\beta \\rVert $. \n",
    "\n",
    "Suppose that the classes overlap in feature space, ie the two classes aren't perfectly linearly separable. Then, we can modify our constraint to still maximize $M$, but allow for points to be on the wrong side of the margin. These points are called slack variables (these are the circle points that aren't on the dotted lines). So, we need to modify our constraint to:\n",
    "\n",
    "$y_i(x_i^T\\beta + 1) \\geq M(1-\\xi_i), i=1, \\dots N$, where $\\xi_i$ are slack variables.\n",
    "\n",
    "This allows us to re-write our problem as: \n",
    "\n",
    "$ \\underset{\\beta}{\\operatorname{min}} \\lVert \\beta \\rVert$\n",
    "\n",
    "subject to: \n",
    "\n",
    "$y_i(x_i^T\\beta + 1) \\geq 1 - \\xi_1 \\forall i$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\xi_i > 0, \\sum \\xi_i \\leq C$, where $C$ is some constant. \n",
    "\n",
    "Finally, we can re-formulate our problem as a computationally efficient quadratic program in matricial form as:\n",
    "\n",
    "$argmin \\; \\lVert \\beta \\rVert^2  + C1^T\\xi$ \n",
    "\n",
    "subject to: \n",
    "\n",
    "$ \\xi \\geq diag(y)X\\beta + 1$\n",
    "\n",
    "and \n",
    "\n",
    "$\\xi > 0$\n",
    "Where $diag(y)$ denotes a diagonal matrix with the elements of $y$ on its diagonal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
